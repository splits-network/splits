---
phase: 01-search-infrastructure
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - supabase/migrations/YYYYMMDD000002_search_index_triggers_core.sql
autonomous: true

must_haves:
  truths:
    - "Inserting/updating a candidate immediately creates/updates a row in search.search_index"
    - "Inserting/updating a job immediately creates/updates a row in search.search_index"
    - "Inserting/updating a company immediately creates/updates a row in search.search_index"
    - "Updating a company name cascades to job entries in search.search_index"
    - "Each search_index row has populated title, subtitle, context, search_vector, and metadata"
  artifacts:
    - path: "supabase/migrations/YYYYMMDD000002_search_index_triggers_core.sql"
      provides: "Trigger functions + triggers for candidates, jobs, companies syncing to search.search_index"
      contains: "sync_candidate_to_search_index"
  key_links:
    - from: "public.candidates trigger"
      to: "search.search_index"
      via: "sync_candidate_to_search_index trigger function"
      pattern: "INSERT INTO search.search_index"
    - from: "public.jobs trigger"
      to: "search.search_index"
      via: "sync_job_to_search_index trigger function"
      pattern: "INSERT INTO search.search_index"
    - from: "public.companies trigger"
      to: "search.search_index"
      via: "sync_company_to_search_index trigger function"
      pattern: "INSERT INTO search.search_index"
---

<objective>
Create trigger-based sync from candidates, jobs, and companies tables to search.search_index. These are the three "primary" entity types with straightforward column mappings.

Purpose: Establishes real-time sync for core entity types so that any INSERT/UPDATE on these tables automatically populates the unified search index.
Output: Migration file with 3 trigger functions, 3 triggers, and backfill statements for existing data.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-search-infrastructure/01-01-SUMMARY.md

Key reference for existing patterns:
@supabase/migrations/20240101000000_baseline.sql (lines 137-280 for build_*_search_vector functions -- use same field mapping)
@docs/search/scalable-search-architecture.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create trigger functions and triggers for candidates, jobs, companies</name>
  <files>supabase/migrations/YYYYMMDD000002_search_index_triggers_core.sql</files>
  <action>
  Create a new migration file (timestamp after Plan 01's migration, e.g., `20260213000003_search_index_triggers_core.sql`).

  For EACH of the 3 entity types (candidates, jobs, companies), create:

  **A) A trigger function** that maps entity columns to the search_index schema and UPSERTs.

  The pattern for each trigger function:
  ```sql
  CREATE OR REPLACE FUNCTION search.sync_ENTITY_to_search_index() RETURNS trigger AS $$
  BEGIN
      INSERT INTO search.search_index (entity_type, entity_id, title, subtitle, context, search_vector, metadata, organization_id, updated_at)
      VALUES (
          'ENTITY_TYPE',
          NEW.id,
          TITLE_EXPRESSION,
          SUBTITLE_EXPRESSION,
          CONTEXT_EXPRESSION,
          SEARCH_VECTOR_EXPRESSION,
          METADATA_JSONB,
          ORGANIZATION_ID_EXPRESSION,
          now()
      )
      ON CONFLICT (entity_type, entity_id)
      DO UPDATE SET
          title = EXCLUDED.title,
          subtitle = EXCLUDED.subtitle,
          context = EXCLUDED.context,
          search_vector = EXCLUDED.search_vector,
          metadata = EXCLUDED.metadata,
          organization_id = EXCLUDED.organization_id,
          updated_at = EXCLUDED.updated_at;
      RETURN NEW;
  END;
  $$ LANGUAGE plpgsql;
  ```

  **B) A trigger** on the source table: AFTER INSERT OR UPDATE, FOR EACH ROW.

  **C) A backfill** for existing rows using INSERT ... SELECT ... ON CONFLICT.

  ---

  **1. CANDIDATES** (entity_type: 'candidate')

  Column mapping:
  - `title`: `NEW.full_name` (the candidate's name)
  - `subtitle`: Concatenate current_title + current_company + location. Example: `'Senior Engineer at Acme Corp - San Diego'`. Use COALESCE and trim empties:
    ```sql
    COALESCE(
        NULLIF(
            CONCAT_WS(' - ',
                NULLIF(CONCAT_WS(' at ', NULLIF(NEW.current_title, ''), NULLIF(NEW.current_company, '')), ''),
                NULLIF(NEW.location, '')
            ),
            ''
        ),
        ''
    )
    ```
  - `context`: Concatenate all searchable text fields: `CONCAT_WS(' ', NEW.full_name, NEW.email, NEW.current_title, NEW.current_company, NEW.skills, NEW.bio, NEW.location, NEW.phone, NEW.desired_job_type, NEW.linkedin_url, NEW.github_url, NEW.portfolio_url)`
  - `search_vector`: Build using the SAME weight scheme as `build_candidates_search_vector()`:
    ```sql
    setweight(to_tsvector('english', COALESCE(NEW.full_name, '')), 'A') ||
    setweight(to_tsvector('english', COALESCE(NEW.email, '')), 'B') ||
    setweight(to_tsvector('english', COALESCE(NEW.current_title, '')), 'B') ||
    setweight(to_tsvector('english', COALESCE(NEW.current_company, '')), 'B') ||
    setweight(to_tsvector('english', COALESCE(NEW.skills, '')), 'B') ||
    setweight(to_tsvector('english', COALESCE(NEW.bio, '')), 'B') ||
    setweight(to_tsvector('english', COALESCE(NEW.location, '')), 'C') ||
    setweight(to_tsvector('english', COALESCE(NEW.phone, '')), 'C') ||
    setweight(to_tsvector('english', COALESCE(NEW.desired_job_type, '')), 'C') ||
    setweight(to_tsvector('english', COALESCE(NEW.linkedin_url, '')), 'D') ||
    setweight(to_tsvector('english', COALESCE(NEW.github_url, '')), 'D') ||
    setweight(to_tsvector('english', COALESCE(NEW.portfolio_url, '')), 'D')
    ```
  - `metadata`: `jsonb_build_object('email', NEW.email, 'location', NEW.location, 'current_title', NEW.current_title, 'current_company', NEW.current_company, 'skills', NEW.skills)`
  - `organization_id`: NULL (candidates are not org-scoped; access control is applied at query time via access context)

  **2. JOBS** (entity_type: 'job')

  Column mapping:
  - `title`: `NEW.title`
  - `subtitle`: `CONCAT_WS(' - ', NULLIF(NEW.company_name, ''), NULLIF(NEW.location, ''))` -- Jobs table already has denormalized company_name
  - `context`: `CONCAT_WS(' ', NEW.title, NEW.description, NEW.recruiter_description, NEW.candidate_description, NEW.company_name, NEW.company_industry, NEW.company_headquarters_location, NEW.location, NEW.employment_type, NEW.department, NEW.status)`
  - `search_vector`: Use the same weight scheme as `build_jobs_search_vector()`. Note: jobs has a 12-param version that includes requirements_text. For the search_index trigger, include the same fields but skip inline requirements query (the existing trigger on jobs already handles requirements via a separate trigger that updates search_vector -- we can re-use NEW.search_vector from the jobs table itself if it's already populated).

  ACTUALLY -- simplest approach: Since the jobs table ALREADY has a populated `search_vector` column maintained by its own triggers, just COPY it:
    ```sql
    NEW.search_vector  -- reuse the already-computed tsvector from the jobs table
    ```
  This avoids duplicating the complex requirements-aware vector logic. The jobs triggers fire BEFORE INSERT/UPDATE and populate search_vector, then our AFTER trigger reads it.

  IMPORTANT: Make this an AFTER trigger (not BEFORE) so that `NEW.search_vector` is already populated by the existing `update_jobs_search_vector_trigger` (which is a BEFORE trigger).

  - `metadata`: `jsonb_build_object('company_name', NEW.company_name, 'location', NEW.location, 'employment_type', NEW.employment_type, 'department', NEW.department, 'status', NEW.status, 'company_industry', NEW.company_industry)`
  - `organization_id`: `NEW.company_id` (jobs are scoped to companies)

  Also need a cascade trigger: When company name/industry/location changes, the jobs table triggers already update the jobs' denormalized fields + search_vector. But we also need to update the search_index entries for those jobs. Create:
  ```sql
  CREATE OR REPLACE FUNCTION search.cascade_company_to_job_search_index() RETURNS trigger AS $$
  BEGIN
      -- When a company changes, re-sync all its jobs to search_index
      -- The jobs table already has sync triggers that update company_name etc.
      -- We just need to update search_index with the new subtitle/metadata
      UPDATE search.search_index si
      SET
          subtitle = CONCAT_WS(' - ', NULLIF(j.company_name, ''), NULLIF(j.location, '')),
          metadata = jsonb_build_object('company_name', j.company_name, 'location', j.location, 'employment_type', j.employment_type, 'department', j.department, 'status', j.status, 'company_industry', j.company_industry),
          search_vector = j.search_vector,
          updated_at = now()
      FROM public.jobs j
      WHERE si.entity_type = 'job'
        AND si.entity_id = j.id
        AND j.company_id = NEW.id;
      RETURN NEW;
  END;
  $$ LANGUAGE plpgsql;

  CREATE OR REPLACE TRIGGER cascade_company_to_job_search_index
  AFTER UPDATE OF name, industry, headquarters_location ON public.companies
  FOR EACH ROW
  EXECUTE FUNCTION search.cascade_company_to_job_search_index();
  ```

  **3. COMPANIES** (entity_type: 'company')

  Column mapping:
  - `title`: `NEW.name`
  - `subtitle`: `CONCAT_WS(' - ', NULLIF(NEW.industry, ''), NULLIF(NEW.headquarters_location, ''))`
  - `context`: `CONCAT_WS(' ', NEW.name, NEW.description, NEW.industry, NEW.headquarters_location, NEW.company_size, NEW.website)`
  - `search_vector`: Reuse `NEW.search_vector` (same approach as jobs -- the companies table already has a populated search_vector from its own BEFORE trigger)
  - `metadata`: `jsonb_build_object('industry', NEW.industry, 'headquarters_location', NEW.headquarters_location, 'company_size', NEW.company_size, 'website', NEW.website)`
  - `organization_id`: `NEW.id` (the company IS the organization)

  ---

  **Trigger timing:** ALL sync-to-search-index triggers must be AFTER INSERT OR UPDATE (not BEFORE), because they need to read `NEW.search_vector` which is populated by the existing BEFORE triggers on each table.

  **Backfill statements** at the end of the migration:
  ```sql
  -- Backfill candidates
  INSERT INTO search.search_index (entity_type, entity_id, title, subtitle, context, search_vector, metadata, organization_id, updated_at)
  SELECT 'candidate', id, full_name, ..., search_vector, ..., NULL, now()
  FROM public.candidates
  ON CONFLICT (entity_type, entity_id) DO UPDATE SET ...;

  -- Backfill jobs
  INSERT INTO search.search_index (entity_type, entity_id, title, subtitle, context, search_vector, metadata, organization_id, updated_at)
  SELECT 'job', id, title, ..., search_vector, ..., company_id, now()
  FROM public.jobs
  ON CONFLICT (entity_type, entity_id) DO UPDATE SET ...;

  -- Backfill companies
  INSERT INTO search.search_index (entity_type, entity_id, title, subtitle, context, search_vector, metadata, organization_id, updated_at)
  SELECT 'company', id, name, ..., search_vector, ..., id, now()
  FROM public.companies
  ON CONFLICT (entity_type, entity_id) DO UPDATE SET ...;
  ```

  **Delete triggers:** Also add AFTER DELETE triggers to remove rows from search_index:
  ```sql
  CREATE OR REPLACE FUNCTION search.delete_from_search_index() RETURNS trigger AS $$
  BEGIN
      DELETE FROM search.search_index
      WHERE entity_type = TG_ARGV[0]
        AND entity_id = OLD.id;
      RETURN OLD;
  END;
  $$ LANGUAGE plpgsql;

  CREATE TRIGGER delete_candidate_from_search_index
  AFTER DELETE ON public.candidates FOR EACH ROW
  EXECUTE FUNCTION search.delete_from_search_index('candidate');

  CREATE TRIGGER delete_job_from_search_index
  AFTER DELETE ON public.jobs FOR EACH ROW
  EXECUTE FUNCTION search.delete_from_search_index('job');

  CREATE TRIGGER delete_company_from_search_index
  AFTER DELETE ON public.companies FOR EACH ROW
  EXECUTE FUNCTION search.delete_from_search_index('company');
  ```
  </action>
  <verify>
  Review the SQL file for syntax correctness. Verify:
  - All 3 sync trigger functions exist (sync_candidate_to_search_index, sync_job_to_search_index, sync_company_to_search_index)
  - All 3 sync triggers are AFTER INSERT OR UPDATE
  - Delete trigger function exists with TG_ARGV pattern
  - All 3 delete triggers exist
  - Company cascade trigger exists for updating job search_index entries
  - All 3 backfill INSERT...SELECT statements exist
  - ON CONFLICT clauses use (entity_type, entity_id)
  </verify>
  <done>
  - Migration file exists with trigger functions for candidates, jobs, companies
  - Each trigger maps entity columns to search_index schema (title, subtitle, context, search_vector, metadata, organization_id)
  - UPSERT pattern using ON CONFLICT (entity_type, entity_id)
  - Delete triggers clean up search_index when source rows are deleted
  - Company cascade trigger updates job entries when company data changes
  - Backfill statements populate search_index from existing data
  - INFRA-02 (candidates), INFRA-03 (jobs), INFRA-04 (companies) complete
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify search_index population with SQL queries</name>
  <files>none (verification only)</files>
  <action>
  After the migration file is created, write verification SQL queries as comments at the bottom of the migration file (inside a block comment) that can be run manually to verify the triggers work:

  ```sql
  /*
  -- VERIFICATION QUERIES (run manually after migration):

  -- 1. Check search_index has rows for all 3 entity types
  SELECT entity_type, COUNT(*) as count
  FROM search.search_index
  GROUP BY entity_type
  ORDER BY entity_type;

  -- 2. Verify a candidate entry has correct structure
  SELECT entity_type, entity_id, title, subtitle, metadata, length(search_vector::text) as vector_length
  FROM search.search_index
  WHERE entity_type = 'candidate'
  LIMIT 3;

  -- 3. Verify a job entry has correct structure
  SELECT entity_type, entity_id, title, subtitle, metadata->>'company_name' as company, metadata->>'status' as status
  FROM search.search_index
  WHERE entity_type = 'job'
  LIMIT 3;

  -- 4. Verify a company entry has correct structure
  SELECT entity_type, entity_id, title, subtitle, metadata->>'industry' as industry
  FROM search.search_index
  WHERE entity_type = 'company'
  LIMIT 3;

  -- 5. Test cross-entity search with ts_rank
  SELECT entity_type, title, subtitle, ts_rank(search_vector, websearch_to_tsquery('english', 'engineer')) as rank
  FROM search.search_index
  WHERE search_vector @@ websearch_to_tsquery('english', 'engineer')
  ORDER BY rank DESC
  LIMIT 10;

  -- 6. Verify organization_id mapping
  SELECT entity_type, COUNT(*) as total,
         COUNT(organization_id) as with_org_id,
         COUNT(*) - COUNT(organization_id) as without_org_id
  FROM search.search_index
  GROUP BY entity_type;
  */
  ```

  These verification queries demonstrate that the search_index is properly populated and queryable.
  </action>
  <verify>
  Verification queries exist as comments in the migration file.
  The queries test: row counts per entity type, correct field mapping, cross-entity ts_rank search, organization_id presence.
  </verify>
  <done>
  - Verification SQL queries are documented in the migration file
  - Queries cover all 3 entity types and cross-entity search
  </done>
</task>

</tasks>

<verification>
1. Migration file exists and is syntactically valid SQL
2. 3 trigger functions created in `search` schema
3. 3 AFTER INSERT OR UPDATE triggers on candidates, jobs, companies
4. 3 AFTER DELETE triggers for cleanup
5. 1 cascade trigger for company->job updates
6. 3 backfill INSERT...SELECT statements
7. Verification queries documented
</verification>

<success_criteria>
- Trigger functions correctly map each entity's columns to search_index schema
- UPSERT pattern handles both new inserts and updates
- Backfill populates search_index from existing data
- Cross-entity search via ts_rank is possible on search_index
- INFRA-02 (candidates sync), INFRA-03 (jobs sync), INFRA-04 (companies sync) complete
</success_criteria>

<output>
After completion, create `.planning/phases/01-search-infrastructure/01-02-SUMMARY.md`
</output>
